## qwen3-4b é‡‡ç”¨tokenizer-bpe



# ğŸ§­ æ€»è§ˆï¼šä¸€ä¸ªæ¨¡å‹çš„ tokenizer ä¿¡æ¯åœ¨å“ªé‡Œï¼Ÿ

åœ¨ HuggingFace æ¨¡å‹ä»“åº“é‡Œï¼Œ**ä¸ tokenizer ç›¸å…³çš„æ–‡ä»¶é€šå¸¸æœ‰è¿™äº›**ï¼š

| æ–‡ä»¶å                         | ä½œç”¨                                                | å¿…çœ‹ç¨‹åº¦  |
| --------------------------- | ------------------------------------------------- | ----- |
| **tokenizer.json**          | æœ€é‡è¦ï¼åŒ…å«å®Œæ•´ tokenizerï¼švocabã€mergesã€è§„åˆ™                | â­â­â­â­â­ |
| **tokenizer_config.json**   | æŒ‡å®š tokenizer ç±»å‹ï¼ˆBPE / SentencePieceï¼‰ã€ç‰¹æ®Š token é…ç½®ç­‰ | â­â­â­â­  |
| **vocab.json**              | token â†’ id æ˜ å°„ï¼ˆBPE tokenizer çš„ vocabï¼‰              | â­â­â­   |
| **merges.txt**              | BPE çš„ merge è§„åˆ™                                    | â­â­â­   |
| **special_tokens_map.json** | ç‰¹æ®Š token çš„åç§°å’Œ idï¼ˆæœ‰äº›æ¨¡å‹æœ‰ï¼‰                           | â­â­    |
| **config.json**             | æ¨¡å‹ç»“æ„ï¼Œä¸å±äº tokenizer ä½† tokenizer ä¸ä¹‹å¯¹é½               | â­     |

åœ¨ Qwen3-4B ä¸­ä½ ä¼šçœ‹åˆ°ï¼š

```
tokenizer.json          â† æœ€é‡è¦
tokenizer_config.json
vocab.json
merges.txt              â† BPE ä¿¡æ¯
```

è¿™äº›æ–‡ä»¶æ„æˆäº† Qwen3-4B çš„ tokenizer å®šä¹‰ã€‚

---

# ğŸ§µ Step 1ï¼šå…ˆçœ‹ tokenizer_config.jsonï¼ˆå®ƒå‘Šè¯‰ä½ â€œæ˜¯ä»€ä¹ˆç±»å‹â€ï¼‰

`tokenizer_config.json` å†³å®šäº† **tokenizer çš„ç®—æ³•ç±»å‹**ã€‚

æ¯”å¦‚ Qwen3 çš„å†…å®¹å¤§æ¦‚æ˜¯è¿™æ ·ï¼ˆç¤ºä¾‹ç»“æ„ï¼‰ï¼š

```json
{
  "tokenizer_class": "Qwen2Tokenizer",
  "model_max_length": 32768,
  "padding_side": "right",
  "truncation_side": "right",
  "bos_token": "<|im_start|>",
  "eos_token": "<|im_end|>",
  "unk_token": "<unk>",
  "pad_token": "<pad>"
}
```

å…³é”®ç‚¹ï¼š

### âœ” tokenizer_class = Qwen2Tokenizer

è¯´æ˜å®ƒæ˜¯ Qwen è‡ªå·±åŸºäº **BPE çš„ tokenizer**ã€‚

### âœ” å®šä¹‰ special tokens

* `<|im_start|>`
* `<|im_end|>`
* `<unk>`
* `<pad>`

è¿™äº› token çš„ **id ä¸æ˜¯åœ¨è¿™é‡Œå®šä¹‰**ï¼Œè€Œæ˜¯åœ¨ `tokenizer.json` ä¸­å®šä¹‰ã€‚

---

# ğŸ§µ Step 2ï¼šçœ‹ vocab.jsonï¼ˆtoken â†’ idï¼‰

è¿™æ˜¯ BPE vocab æ–‡ä»¶ã€‚

ä¸¾ä¾‹ï¼ˆç‰‡æ®µï¼‰ï¼š

```json
{
  "<unk>": 0,
  "<bos>": 1,
  "<eos>": 2,
  "ä½ ": 5321,
  "å¥½": 9851,
  "ä¸–ç•Œ": 11002,
  ...
}
```

è¿™æ˜¯ä¸€ä¸ª **ç®€å•çš„å­—å…¸**ï¼š

* keyï¼štokenï¼ˆå­—ç¬¦ä¸²ï¼‰
* valueï¼šidï¼ˆæ•´æ•°ï¼‰

æ¨¡å‹çš„ Embedding çŸ©é˜µä¸­ **ç¬¬ i è¡Œå°±æ˜¯ id=i çš„ token çš„å‘é‡**ã€‚

**æ³¨æ„**ï¼š
å®é™… Qwen3 vocab.json æœ‰ 2.7MBï¼Œå¾ˆå¤§ï¼Œä½†ç»“æ„åŒæ ·æ˜¯ï¼š

```
token string  â†’ token id
```

---

# ğŸ§µ Step 3ï¼šçœ‹ merges.txtï¼ˆBPE çš„æ ¸å¿ƒï¼šå­è¯åˆå¹¶è§„åˆ™ï¼‰

è¿™æ˜¯ BPE tokenizer æœ€å…³é”®çš„æ–‡ä»¶ä¹‹ä¸€ã€‚

å®ƒé•¿è¿™æ ·ï¼š

```
a b
ä½  å¥½
ä¸– ç•Œ
...
```

æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ª mergeï¼š

```
(tokenA, tokenB) â†’ tokenAB
```

BPE çš„å·¥ä½œæ–¹å¼ï¼š

1. è¾“å…¥æ–‡æœ¬ä¼šå…ˆåˆ†æˆæœ€å°å•å…ƒï¼ˆå­—èŠ‚æˆ–å­—ç¬¦ï¼‰
2. æŒ‰ merges.txt çš„é¡ºåºä¾æ¬¡å°è¯•åˆå¹¶
3. ç›´åˆ°æ— æ³•ç»§ç»­åˆå¹¶

æ¯”å¦‚ï¼š

è¾“å…¥ `"ä½ å¥½ä¸–ç•Œ"`
åˆå§‹åˆ‡æˆï¼š

```
["ä½ ", "å¥½", "ä¸–", "ç•Œ"]
```

å¦‚æœ merges.txt ç¬¬ 1 æ¬¡åˆå¹¶æ˜¯ï¼š

```
ä½  å¥½   â†’   ä½ å¥½
```

ç¬¬ 2 æ¬¡ï¼š

```
ä¸– ç•Œ   â†’   ä¸–ç•Œ
```

åˆ™æœ€ç»ˆ tokenizeï¼š

```
["ä½ å¥½", "ä¸–ç•Œ"]
```

ç„¶åæŸ¥ vocab.json å¾—åˆ° idï¼š

```
[5321, 11002]
```

---

# ğŸ§µ Step 4ï¼štokenizer.jsonï¼ˆæœ€å…³é”®çš„æ€»æ–‡ä»¶ï¼‰

Huggingface çš„ç°ä»£ tokenizer ç»“æ„å…¨éƒ¨æ‰“åŒ…è¿›è¿™ä¸ªæ–‡ä»¶ã€‚

é‡Œé¢åŒ…å«ï¼š

| é¡¹ç›®                               | è¯´æ˜                |
| -------------------------------- | ----------------- |
| modelï¼ˆBPE/SentencePiece/Unigramï¼‰ | tokenizer ç±»å‹      |
| vocab                            | tokenâ†’idã€idâ†’token |
| mergesï¼ˆå¦‚æœæ˜¯ BPEï¼‰                  | åˆå¹¶è§„åˆ™              |
| pre_tokenizer                    | é¢„åˆ‡åˆ†è§„åˆ™ï¼ˆç©ºæ ¼ã€å­—èŠ‚ç­‰ï¼‰     |
| decoder                          | åå‘ decode è§„åˆ™      |

ä½ å¯ä»¥æŠŠå®ƒç†è§£æˆ**å®Œæ•´ tokenizer çš„äºŒè¿›åˆ¶/JSON åŒ–å®šä¹‰**ã€‚

çœ‹ä¸€å°æ®µç»“æ„ï¼ˆç¤ºä¾‹ï¼‰ï¼š

```json
{
  "model": {
    "type": "BPE",
    "vocab": {
      "<unk>": 0,
      "<bos>": 1,
      "ä½ ": 5321,
      ...
    },
    "merges": [
      "ä½  å¥½",
      "ä¸– ç•Œ",
      ...
    ]
  },
  "pre_tokenizer": {
    "type": "ByteLevel"
  },
  "decoder": {
    "type": "ByteLevel",
    "add_prefix_space": false
  }
}
```

æ‰€ä»¥ï¼š

# ğŸ‘‰ **åªçœ‹ tokenizer.json å°±èƒ½å®Œå…¨æ¢å¤ tokenizer**

å…¶ä»–æ–‡ä»¶åªæ˜¯è¾…åŠ©ç†è§£ã€‚

---

# ğŸ§µ Step 5ï¼šå¦‚ä½•å®é™…åŠ è½½ tokenizerï¼Ÿ

ä½ å¯ä»¥ç›´æ¥è¿™æ ·ï¼š

```python
from transformers import AutoTokenizer

tok = AutoTokenizer.from_pretrained("Qwen/Qwen3-4B")

print(tok.encode("ä½ å¥½ä¸–ç•Œ"))
```

è¾“å‡ºç±»ä¼¼ï¼š

```
[151643, 151805]
```

å¹¶ä¸”ï¼š

```python
print(tok.convert_ids_to_tokens([151643, 151805]))
```

ç»“æœä¼šæ˜¯ï¼š

```
["ä½ å¥½", "ä¸–ç•Œ"]
```

