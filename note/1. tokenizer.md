## 1. 什么是tokenizer?

🔍 Tokenizer 的功能 = 文本 ↔ token id 的映射

LLM 不能直接处理文字，只能处理数字序列。
Tokenizer 的作用就是：

文字 → token → token id（整数）

例子：

"你好世界"
↓ （分词）
["你", "好", "世界"]
↓ （查 vocab）
[1234, 5678, 91011]


不同模型（LLaMA / GPT / Qwen）使用不同的 tokenizer，所以：

👉 tokenizer 必须和模型权重配套

## 2. 以qwen3-4b为例，讲解tokenizer的完整过程

Tokenizer 在推理时具体做了什么？


### 选择“切分粒度”：字符？单词？子词？

从简单到复杂，大致三种：

#### ✅ 1）字符级 tokenizer（最容易自己实现）

思路：**每个 Unicode 字符就是一个 token**。

* vocab = 所有字符 + 特殊 token
* `"你好世界"` → `["你", "好", "世", "界"]`
* 实现极其简单，但句子会很长，模型效率差。


#### ✅ 2）单词级 tokenizer（适合空格分词语言）

英语里常见，但中文就不合适，因为中文写作没有空格分词；这里略过。

---

#### ✅ 3）子词级 tokenizer（BPE / SentencePiece）——实际模型都用这个

现代大模型（GPT、LLaMA、Qwen 等）基本上都是某种**子词（subword）**方法：

* BPE（Byte Pair Encoding）
* Unigram（SentencePiece 的一种算法）
* Byte-level BPE（GPT-2/3/4 类）

核心思想：

> 不再用“词”或“字”作为单位，而是学习一套“子词片段”，让：
>
> * 高频词用少量 token 表示（`"你好世界"` → `"你好", "世界"`）
> * 罕见词可以拆开（`"Transformer"` → `"Trans", "form", "er"`）

以 BPE 为例，你大致需要：

1. 初始 vocab：按字符/byte 初始化；
2. 在大量语料中统计**相邻 token 对（pair）**的频率；
3. 每次选择出现频率最高的 pair，把它合并成新 token；
4. 重复 N 次 → 得到 merges 列表 + vocab。

最后你会有：

```python
vocab: token -> id
merges: List[ (token_a, token_b) ]  # 有顺序的合并规则
```

**编码时**就是应用这些 merges，把输入尽可能按最长匹配合并。

**qwen-4b就是采用BPE模式**
---
