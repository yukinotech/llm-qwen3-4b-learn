下面以 **“猫 在 垫子 上”** 这段训练文案为例，从 **Tokenizer → 数据构造 → 前向计算(含自注意力) → 损失计算** 的顺序讲清楚 LLM 自注意力机制的训练流程。
我也会特别说明为什么 **模型会学到“上”是正确答案**。

---

## 1. Tokenizer：将文本拆成 token

Tokenizer 的作用是把原始文本转成模型能处理的 **离散 token ID 序列**。

训练样例：

```
猫 在 垫子 上
```

例如某个 tokenizer 可能输出：

```
[猫] → 321
[在] → 582
[垫子] → 1440
[上] → 233
```

模型的输入序列就是：

```
[321, 582, 1440, 233]
```

Tokenizer 会构造 **input_ids** 与 **targets（shifted labels）**：

| 输入位置 | 输入 token | 目标 token（预测任务）         |
| ---- | -------- | ---------------------- |
| 0    | 猫        | 在                      |
| 1    | 在        | 垫子                     |
| 2    | 垫子       | 上                      |
| 3    | 上        | （句子结束 token，如 `<eos>`） |

所以 —— **模型并不是直接被告知“上”是答案，而是通过预测下一个 token 学到“上”应该出现。**

---

## 2. 构造训练数据：自回归语言模型任务

训练 LLM 使用自回归语言模型（Auto-regressive LM）目标，即：

> 在位置 **t**，模型要预测位置 **t+1** 的 token（即下一词预测，Next-token prediction）。

因此训练输入是：

```
input_ids = [猫, 在, 垫子, 上]
labels    = [在, 垫子, 上, <eos>]
```

模型训练时会让正确答案 “上” 出现在 **第三个标签位置**（对应输入中的“垫子”）。

---

## 3. 前向传播：Transformer + 自注意力

![Image](https://miro.medium.com/1%2AvrSX_Ku3EmGPyqF_E-2_Vg.png?utm_source=chatgpt.com)

![Image](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png?utm_source=chatgpt.com)

![Image](https://cdn.sanity.io/images/jo7n4k8s/production/25ebbba9d2ce12efc8c3da181942367f05c795be-2386x1338.jpg?auto=format\&utm_source=chatgpt.com)

![Image](https://aiml.com/wp-content/uploads/2023/09/transformers-self-attention-step-by-step-explanation.png?utm_source=chatgpt.com)

Transformer 的每一层做两件事：

1. **自注意力（Self-Attention）**
2. **前馈网络（Feed-Forward Network, FFN）**

### 3.1 Masked Self-Attention（遮罩自注意力）

训练语言模型时使用 **Decoder-only Transformer**，其中自注意力是 **masked attention** —— 只能看见当前 token 之前的内容，不能偷看未来。

例子中，模型在预测“上”时看到的是：

* 猫
* 在
* 垫子

但 **不能看到“上”本身**（因为那是答案）。

### 3.2 自注意力内部发生什么？

以输入 token "垫子" 为例，它会生成：

* Query（Q）
* Key（K）
* Value（V）

然后计算与之前 token 的相似度权重：

```
Attention(垫子) = softmax( Q_垫子 · K_猫,
                            Q_垫子 · K_在,
                            Q_垫子 · K_垫子 )
```

这些注意力权重会决定模型“认为”哪些词更重要。

例如，“垫子” 可能更关注 “在”，因为它们组合成语义模式 “在 垫子 上”。

模型通过 Value 向量加权求和得到上下文表示。

---

## 4. 输出层：预测下一个 token

经过多层 Transformer 后，得到 hidden state：

```
h_t = Transformer(x_0 … x_t)
```

它会被送入线性层 + softmax：

```
P(token | "猫 在 垫子") = softmax(W·h_2)
```

softmax 会输出一个 **词表大小的概率分布**，而 “上” 应该有最高概率。

---

## 5. 损失函数：Cross Entropy（交叉熵）

对于第三个位置（输入是“垫子”），label 是 “上”。
模型预测一个概率分布，其中 “上” 的概率可能是：

```
P(上) = 0.02   （模型初始很差）
```

交叉熵损失会惩罚这种错误预测：

```
Loss = - log P(上)
```

梯度下降会更新所有参数：

* Token embedding
* Q/K/V 投影矩阵
* FFN 参数
* 输出层 W

随着大量训练样本，模型逐渐提高“上”的概率，直到：

```
P(上 | 猫 在 垫子) → 趋近 1.0
```

---

## 6. 为什么模型学到了 “上” 是正确答案？

因为语言模型的目标是：

> **给定前文，预测下一个 token。**

大量中文语料中会出现类似模式：

* “在 垫子 上”
* “在 桌子 上”
* “在 床 上”

模型在训练数据中不断看到这些结构，因此 Q/K/V 权重逐渐学到：

* “在 … 上” 是高频结构
* “垫子” 后的常见词是 “上”
* “在 垫子” → 上，比如语义上对应介词结构

所以 **模型不是死记“上”，而是学到语法与统计规律**。

---

## **流程总结图（文字版）**

```
文本 → Tokenizer → Token IDs
      ↓
构造训练序列 (input_ids, labels)
      ↓
Transformer（多层）
  ├─ Masked Self-Attention
  └─ Feed-Forward
      ↓
线性层 + Softmax
      ↓
预测下一个 token
      ↓
Cross Entropy Loss
      ↓
反向传播更新参数
```

---
