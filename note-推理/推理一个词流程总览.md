 好，我们来一口气走完一遍 **“Qwen3 推理从字到下一个字”** 的全过程。
输入固定为：`"猫在垫子"`，目标是看到模型怎么一步步推理出下一个字（大概率是“上”）。

我按“真实实现的模块顺序”来讲，每一步都说清：**输入/输出是什么、形状是什么、在干嘛**。

---

## 0. 用户输入

你在代码里写的可能是：

```python
text = "猫在垫子"
```

这是一个 **Unicode 字符串**，模型此时还“看不见”，需要 tokenizer 处理。

---

## 1. Tokenizer：文本 → token → input_ids（整数）

### 1.1 文本预处理

对于 base LM 情况（不是 chat 模式），可以直接 encode：

```python
enc = tokenizer(text, add_special_tokens=False)
input_ids = enc["input_ids"]
```

内部做的事大致是：

1. `"猫在垫子"` → 字节 / 字符序列
2. 结合 `merges.txt` 做 BPE 合并
3. 用 `vocab.json` 把每个 token string 映射为 id

### 1.2 结果示意（注意：id 是我随便举例，**示意用**）

比如 tokenizer 得到：

```python
tokens    = ["猫", "在", "垫", "子"]    # 或 ["猫", "在", "垫子"] 之类，看 BPE
input_ids = [1234, 5678, 91011, 1213]  # 假设
seq_len   = 4
```

**这一串 `input_ids: [int,int,...]` 就是接下来所有计算的“起点”。**

---

## 2. 组装模型输入张量

通常我们会让 batch_size = 1：

```python
import torch

input_ids = torch.tensor([[1234, 5678, 91011, 1213]])  # shape: (1, 4)
attention_mask = torch.ones_like(input_ids)            # 全 1，表示都有效
```

* `input_ids` shape: **(batch_size, seq_len) = (1, 4)**
* `attention_mask`: 同形状，做 padding mask 用（这里只有真实 token，没有 pad）

有的实现还会额外算 `position_ids`，但在 RoPE 中通常内部算。

---

## 3. Embedding 层：id → 向量

### 3.1 Token Embedding 查表

模型有一个 embedding 矩阵 `E`：

* 形状大概是 `(vocab_size, hidden_size)`
* 比如 `vocab_size ≈ 150k`, `hidden_size = 3072`（举例）

对于 `input_ids = [1234, 5678, 91011, 1213]`：

```python
# 伪代码
H0 = E[input_ids]  # shape: (1, 4, hidden_size)
```

得到：

* `H0` 的形状：**(batch_size, seq_len, hidden_size) = (1, 4, 3072)**
* 这就是 **4 个 token 的向量表示**。

### 3.2 位置编码（RoPE）

Qwen3 使用 RoPE（旋转位置编码），不会直接加一个 `pos_embedding`，
而是在后面 **算 Q/K 的时候对它们做旋转**，这里先记着：**位置信息晚点注入**。

---

## 4. 进入 Transformer Block 堆栈（N 层）

假设 Qwen3-4B 有 `N` 层（几十层），我们从第 1 层开始：

```python
H = H0  # shape: (1, 4, hidden_size)
for layer in range(N):
    H = TransformerBlock[layer](H, attention_mask)
```

每一层主要分两部分：

1. 多头自注意力（Self-Attention）
2. 前馈网络（MLP）

我们重点看第 1 层的 Attention 怎么处理 `"猫在垫子"` 这 4 个位置。

---

## 5. 一层里：多头自注意力的细节

### 5.1 从 H 投影出 Q、K、V

对当前层的输入 `H`（shape: (1, 4, hidden_size)），用三组权重矩阵：

```python
Q = H @ W_Q   # (1, 4, hidden_size)
K = H @ W_K   # (1, 4, hidden_size)
V = H @ W_V   # (1, 4, hidden_size)
```

然后拆成多头：

```python
Q → (batch, n_heads, seq_len, head_dim)
K → (batch, n_heads, seq_len, head_dim)
V → (batch, n_heads, seq_len, head_dim)
```

比如：

* `n_heads = 32`
* `head_dim = hidden_size / n_heads = 3072 / 32 = 96`

那么：

* Q/K/V 形状：**(1, 32, 4, 96)**

这里的第 3 个维度 `4`，就是 4 个位置：“猫”“在”“垫”“子”。

### 5.2 对 Q/K 应用 RoPE（加入位置信息）

对于第 t 个位置（0~3），RoPE 会把 Q_t 和 K_t 按照位置 t 做一个旋转变换，
本质上是给不同位置的向量加上一个“角度”。

结果还是 **相同形状的 Q, K**，但已经“知道自己是第几个 token 了”。

---

### 5.3 计算注意力分数：QKᵀ → (4×4 的矩阵)

对每个头分别算：

```python
# 伪公式，每个 head 单独算
scores = Q @ K.transpose(-1, -2) / sqrt(head_dim)
# shape: (1, 32, 4, 4)
```

* 最后两个维度 `(4, 4)` 就是：

  * query 位置 i
  * 对所有 key 位置 j 的相似度

以第三个位置（对应“垫”或“在垫子”的最后一个 token）为例：

* 第 3 行的一行 `scores[*, *, 2, :]` 是一个长度 4 的向量：

  * 它表示 **“当前位置”对 “猫”“在”“垫”“子” 的注意分数**。

---

### 5.4 softmax 得到注意力权重

对上面的 `(4, 4)` 矩阵的最后一维做 softmax：

```python
attn_weights = softmax(scores, dim=-1)
# shape: (1, 32, 4, 4)
```

例如，对第三个位置（假设）：

* 第 3 行：`[猫, 在, 垫, 子]` 的注意力权重可能是：

  ```text
  [0.05, 0.10, 0.70, 0.15]
  ```

这里就体现了我们之前讲的：

> 模型学会了 **在“垫子”附近的位置，应该主要关注“垫子”这个 token**。

---

### 5.5 用注意力权重加权 V，得到新的表示

对每个头：

```python
context_head = attn_weights @ V  # (1, 32, 4, 96)
```

解释：

* 对于每个位置 i，
  `context_head[..., i, :] = Σ_j α_{i,j} * V_j`
* 相当于把四个词的信息按“谁更重要”加权平均。

然后把所有 head 拼回去：

```python
context = concat(context_head over heads)  # (1, 4, hidden_size)
context = context @ W_O                    # 再过一个输出线性层
```

最后再加 residual + layernorm：

```python
H1 = LayerNorm(H + context)
```

这就完成了 **一层 self-attention**。

> H1 现在是 “每个位置都看过上下文后”的新向量表示，
> 特别是最后一个位置的向量，已经聚合了“猫在垫子”的整体语义。

---

## 6. MLP 前馈网络

同一层中，H1 还要过一个前馈网络（两个线性层 + 激活）：

```python
intermediate = GELU(H1 @ W1 + b1)
H2 = H1 + (intermediate @ W2 + b2)   # 再 residual
H2 = LayerNorm(H2)
```

* H2 形状仍然是 **(1, 4, hidden_size)**
* 这是这一层的最终输出，作为下一层的输入。

这样反复 N 层：

```python
H_final = H_after_last_layer  # (1, 4, hidden_size)
```

---

## 7. 输出层：从最后一个位置的向量 → 预测下一个 token

模型要预测 “猫在垫子 **[下一字]**”，
所以只关心最后一个位置（位置 index = 3，对应“子”）。

取出最后一列：

```python
h_last = H_final[:, -1, :]   # shape: (1, hidden_size)
```

然后乘上输出权重矩阵（权重可与 embedding 共享或不共享）：

```python
logits = h_last @ W_vocab^T  # shape: (1, vocab_size)
```

* 这里 `W_vocab` 形状大概 `(vocab_size, hidden_size)`
* `logits` 是每个 token 的“未归一化分数”

对 `logits` 做 softmax：

```python
probs = softmax(logits, dim=-1)  # (1, vocab_size)
```

某些 token 的概率可能是：

| token | 概率（示意） |
| ----- | ------ |
| "上"   | 0.62   |
| "里"   | 0.05   |
| "下"   | 0.03   |
| "。"   | 0.01   |
| ...   | ...    |

按解码策略（例如 greedy）：

```python
next_id = probs.argmax(dim=-1)  # 假设对应 "上" 的 id
```

于是模型生成的下一个 token 就是 `"上"`。

---

## 8. 继续生成后续 token（增量推理 + KV Cache）

如果你还想继续生成：

1. 把新 token id 接到序列后面：
   `input_ids = [1234, 5678, 91011, 1213, next_id]`
2. **增量推理时不会重算前面 4 个位置**，
   而是用 KV cache，把之前层里算过的 K/V 存起来，仅对新位置再算一次 Q/K/V。

简略来说：

* 第一次 forward：算 4 个 token 的全量 self-attention
* 之后每次只算“最后一个新 token”，但会用到之前缓存的 K/V
* 这样复杂度从 O(L²) 变成 O(L) per step，推理才不会爆炸。

---

## 9. 总结成一条从左到右的“流水线”

对输入：`"猫在垫子"`

1. **文本**：`"猫在垫子"`
2. **Tokenizer（vocab + merges）**：
   → token: `["猫","在","垫","子"]`
   → `input_ids: [1234, 5678, 91011, 1213]`
3. **组张量**：`input_ids → (1, 4)`，`attention_mask → (1, 4)`
4. **Embedding 查表**：
   → `H0: (1, 4, hidden_size)`
5. **N 层 Transformer Block**：
   每层做：

   * `H → Q,K,V → RoPE → QKᵀ → softmax → 加权 V → context`
   * `H = LayerNorm(H + context)`
   * `H = LayerNorm(H + MLP(H))`
6. **最后一层输出**：`H_final: (1, 4, hidden_size)`
7. **取最后一个位置**：`h_last = H_final[:, -1, :]`
8. **输出层（LM Head）**：
   `logits = h_last @ W_vocab^T → softmax → probs`
9. **解码策略**：
   `next_token_id = argmax(probs)`（或采样）
   → 解码成 `"上"`

> 就这样，Qwen3 从纯字符串 `"猫在垫子"`，一步步变成了下一个 token `"上"` 的概率分布，并选出了它。

---
